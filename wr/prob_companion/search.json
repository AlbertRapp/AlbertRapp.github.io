[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Übungsbegleiter",
    "section": "",
    "text": "Vorwort\nDieser Aufschrieb soll als Begleitinformationen zu den Übungsblättern verstanden werden. Ziel ist es, Inhalte aus den Übungen tiefer zu beleuchten als es das Vorrechnen im regulären Übungsbetrieb erlaubt. Insbesondere versuche ich neben dem rigorosen mathematischen Verständnis aus den Übungsaufgaben ein “intuitives” und weniger formellastiges Verständnis zu fördern.\nGemäß der Natur der Sache müssen einige Inhalte aus dem Skript wiederholt werden. Dennoch kann und soll dieser Aufschrieb das Vorlesungsskript (Spodarev 2020) bzw. die ältere, aber nicht ganz deckungsgleiche Version (Spodarev 2018) nicht ersetzen. Ebenso ist dies nicht als eine Art Zusammenfassung der “wichtigen Prüfungsinhalte” zu verstehen. Es ist lediglich mein Anliegen, einzelne grundlegende Aspekte auf eine weniger formale Weise zu beleuchten, da diese Dinge - meiner Erfahrung nach - notgedrungen durch Zeitmangel und Stoffdichte oftmals im Vorlesungs- und Übungsbetrieb zu kurz kommen.\nIn den vergangenen Semester habe ich versucht, diese Informationen über zusätzliche Folien in den Übungen darzustellen. Damit diese sich allerdings auch ohne meinen zugehörigen Vortrag als Lernunterlage eignen, musste ich einige Inhalte in Textform auf den Folien beschreiben. Ich habe zwar versucht, Texte in den Folien auf ein Minimum zu beschränken, dennoch musste ich - zu meinem eigenen Unwohl - feststellen, dass ich für meinen Geschmack immer noch zu viel Text auf den Folien einbringen musste.\nIch bin einer der größten Kritiker an Vorträgen mit vollgeschriebenen Folien. Leider sind solche Vorträge meiner Erfahrung nach leider eher die Norm als die Ausnahme. Daher bin ich erfreut darüber, diesen Übungsbegleiter als Lernunterlage aushändigen und meine Übungsfolien dadurch schlanker gestalten zu können.\nOb der Übungsbegleiter letztendlich eine sinnvolle Ergänzung zum Übungsbetrieb darstellt, dürft ihr als Übungsteilnehmer selbst entscheiden. Ich bin für Feedback immer offen und nehme auch Kritik (insbesondere bzgl. zu starken Ungenauigkeiten durch die umgangssprachliche Beschreibung mathematischer Inhalte) gerne an. Ihr dürft euch per Mail oder Übungsfeedback im Moodle immer mit euren Anregungen melden.\nAbschließend möchte ich mich an dieser Stelle herzlichst bei Jun.-Prof. Dr. Marco Oesting für die aufmerksame Korrektur dieses Textes bedanken.\n\n\n\n\nSpodarev, Evgeny. 2018. Vorlesungsskript zur Elementaren Wahrscheinlichkeitsrechnung und Statistik.\n\n\n———. 2020. Vorlesungsskript zur Elementaren Wahrscheinlichkeitsrechnung und Stastistik."
  },
  {
    "objectID": "01_Wahrscheinlichkeitsmasse_Mengensysteme.html",
    "href": "01_Wahrscheinlichkeitsmasse_Mengensysteme.html",
    "title": "1  Wahrscheinlichkeitsmaße und Mengensysteme",
    "section": "",
    "text": "Das stochastische Grundgerüst in einem Grundraum \\(E\\) basiert auf Mengen bzw. Ereignissen in einem Wahrscheinlichkeitsraum, der als ein Tripel \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) definiert ist, das die Menge von Elementarereignissen bzw. die Grundmenge \\(\\Omega \\subset E\\) mit einer \\(\\sigma\\)-Algebra \\(\\mathcal{F}\\) und einem Wahrscheinlichkeitsmaß \\(\\mathbb{P}\\) kombiniert. Dabei sind die zwei hier noch unbekannten Begriffe definiert durch\n\nDefinition 1.1 (\\(\\sigma\\)-Algebra) Ein Mengensystem \\(\\mathcal{F} \\subset \\mathcal{P}(\\Omega)\\) heißt \\(\\sigma\\)-Algebra, wenn alle drei der folgenden Bedingungen erfüllt sind:\n\nEs gilt \\(\\emptyset \\in \\mathcal{F}\\).\nFalls \\(A \\in \\mathcal{F}\\), so gilt auch \\(A^c \\in \\mathcal{F}\\).\nFalls \\(A_1, A_2, \\ldots \\in \\mathcal{F}\\) so gilt auch \\(\\cup_{n \\in \\mathbb{N}} A_n \\in \\mathcal{F}\\).\n\n\n\nDefinition 1.2 (Wahrscheinlichkeitsmaß) Sei \\(\\mathcal{F}\\) eine \\(\\sigma\\)-Algebra auf \\(\\Omega\\). Eine Mengenfunktion \\(\\mathbb{P}: \\mathcal{F} \\rightarrow \\mathbb{R}\\) heißt Wahrscheinlichkeitsmaß auf \\((\\Omega, \\mathcal{F})\\), wenn\n\n\\(\\mathbb{P}(A) \\in [0, 1]\\) für alle \\(A \\in \\mathcal{F}\\),\n\\(\\mathbb{P}(\\Omega) = 1\\) und\nFalls \\(A_1, A_2, \\ldots \\in \\mathcal{F}\\) paarweise disjunkt sind, so gilt \\[\\begin{align*}\n  \\mathbb{P} \\bigg( \\bigcup_{n = 1}^{\\infty} A_n \\bigg)\n  =\n  \\sum_{n = 1}^{\\infty} \\mathbb{P}(A_n).\n\\end{align*}\\]\n\n\nDie \\(\\sigma\\)-Algebra dient dabei als Sammelsurium von Mengen, denen wir eine Wahrscheinlichkeit zuordnen können und die wir daher als messbar bezeichnen. Der zentrale Punkt, warum wir das Konstrukt der \\(\\sigma\\)-Algebra brauchen und uns bei dem zugrundeliegenden Mengensystem nicht mit der Potenzmenge \\(\\mathcal{P}(\\Omega)\\) begnügen können, ist der Satz von Vitali. Grob formuliert besagt dieser, dass es Mengen gibt, die nicht messbar sind, d.h. diesen Mengen kann man kein sinnvolles Maß/Volumen zuordnen.\nStattdessen beschränkt man sich auf einige wenige Axiome, die messbare Mengen erfüllen sollten und landet damit bei der Definition der \\(\\sigma\\)-Algebra, deren Bedingungen sich aus stochastischer Perspektive übersetzen lassen als\n\nEinem unmöglichen Ereignis kann man eine Wahrscheinlichkeit zuordnen.\nFalls man einem Ereignis eine Wahrscheinlichkeit zuordnen kann, so kann man auch dem Gegenereignis eine Wahrscheinlichkeit zuordnen.\nLassen sich einer abzählbaren Anzahl von Ereignissen jeweils eine Wahrscheinlichkeit zuordnen, so lässt sich auch dem Ereignis, dass irgendeines dieser Ereignisse eintritt, eine Wahrscheinlichkeit zuordnen.\n\nBei den bisherigen Überlegungen haben wir lediglich die Tatsache untersucht, ob man eine Wahrscheinlichkeit zuordnen kann und haben auf diese Weise die \\(\\sigma\\)-Algebra beleuchtet. Jedoch kann man mit der \\(\\sigma\\)-Algebra alleine keine sinnvolle Aussage über die Höhe der Wahrscheinlichkeit eines Ereignisses treffen.\nDafür benötigen wir eine Funktion, die genau diese Aufgabe erledigt und auch hier gehen wir wieder axiomatisch vor, damit die entsprechende Funktion möglichst intuitive Eigenschaften besitzt. Gerade diese Eigenschaften finden sich in der Definition des Wahrscheinlichkeitsmaßes und lassen sich wieder aus stochastischer Perspektive übersetzen als\n\nJede Wahrscheinlichkeit liegt zwischen \\(0\\) und \\(1\\).\nDie Wahrscheinlichkeit, dass irgendetwas passiert ist 1.\nFalls man beliebig viele Ereignisse betrachtet, die nicht gleichzeitig eintreten können, so ergibt sich die Wahrscheinlichkeit für das Ereignis, dass eins dieser Ereignisse eintritt, als die Summe der Wahrscheinlichkeiten der einzelnen Ereignisse.\n\nBetrachten wir nun zwei elementare Wahrscheinlichkeitsräume.\n\nBeispiel 1.1 Sei \\(\\Omega \\subset \\mathbb{R}\\) und \\(A\\) eine nichtleere Teilmenge von \\(\\Omega\\). Dann ist \\((\\Omega, \\{ \\emptyset, \\Omega, A, A^c \\}, \\mathbb{P})\\) mit \\[\\begin{align*}\n           \\mathbb{P}(B) = \\begin{cases}\n               0 &, B = \\emptyset \\\\\n               1 &, B = \\Omega \\\\\n               p&, B = A \\\\\n               1 - p &,B = A^c\n           \\end{cases}\n\\end{align*}\\]\nein Wahrscheinlichkeitsraum, falls \\(p \\in [0,1]\\).\n\n\nBeispiel 1.2 (Borelsche \\(\\sigma\\)-Algebra) Es sei \\(\\mathcal{B}([0,1])\\) die Borel-\\(\\sigma\\)-Algebra auf \\([0,1]\\). Grob (und sehr vereinfacht) gesagt beinhaltet die Borel-\\(\\sigma\\)-Algebra fast alle Mengen, die irgendwie von Interesse sein könnten,. In diesem Fall beschränken wir uns dabei auf Teilmengen aus dem Intervall \\([0, 1]\\). Zeitgleich sind diese Mengen so “gutartig”, dass man ihnen einen Volumen bzgl. des Lebegue-Maßes \\(\\lambda\\) zuordnen kann.\nDas Lebesgue-Maß ist hierbei ein elementares Maß, das einem Intervall dessen Länge als Maß zuordnet, d.h. \\(\\lambda([a, b]) = b - a\\) für alle \\(a < b\\). Es gilt nun, dass \\(( [0,1],~\\mathcal{B}([0,1]),~\\lambda)\\) ein Wahrscheinlichkeitsraum ist."
  },
  {
    "objectID": "02_Bed_Wsk_und_Bayes.html",
    "href": "02_Bed_Wsk_und_Bayes.html",
    "title": "2  Bedingte Wahrscheinlichkeit und der Satz von Bayes",
    "section": "",
    "text": "Idealerweise ist ein medizinischer Test immer zu 100% zuverlässig. Allerdings wissen wir aus der Praxis, dass es immer wieder zu falsch-positiven oder falsch-negativen Testergebnissen in der Medizin kommt. Das beste worauf wir also hoffen können, ist das ein Test mit sehr großer Wahrscheinlichkeit richtig liegt. Somit sollte ein Test so gestaltet sein, dass die Wahrscheinlichkeit, dass ein Patient tatsächlich krank ist, wenn der Test dies anzeigt, sehr groß ist.\nIntuitiv ist das alles sehr einleuchtend. Allerdings sollten wir das auch noch mathematisch formal präzisieren. Offensichtlich können wir die gesuchte Wahrscheinlichkeit nicht einfach mit \\(\\mathbb{P}(K)\\) bezeichnen, wobei \\(K\\) für das Ereignis steht, dass ein zufällig ausgewählter Mensch krank ist. Schließlich haben wir zusätzliche Information. Unser Test war ja positiv. Dementsprechend würden wir erwarten, dass es nun wahrscheinlicher ist, dass der Patient krank.\nEin naiver Ansatz wäre nun also zu vermuten, dass die gesuchte Wahrscheinlichkeit \\(\\mathbb{P}(K \\cap T)\\) entspricht, wobei \\(T\\) dem Ereignis “Test positiv” und der Schnitt der Mengen dem Ereignis “Beide Ereignisse eingetreten” entspricht. Offensichtlich stellt man schnell fest, dass \\(\\mathbb{P}(K \\cap T) \\leq \\mathbb{P}(K)\\), was wiederum bedeuten würde, dass wir es für weniger wahrscheinlich halten, krank zu sein, nachdem der Test positiv ausgefallen ist. Das kann nicht unserer Intuition entsprechen.\nAllerdings können wir uns überlegen, dass wir zusätzlich zu unseren vorherigen Überlegung den Grundraum auf die Fälle einschränken können, in denen \\(T\\) wahr ist. In diesem Fall muss ich die Wahrscheinlichkeit \\(\\mathbb{P}(K \\cap T)\\) nur noch entsprechend der Wahrscheinlichkeit des neuen Grundraumes normieren, was uns die gesuchte Wahrscheinlichkeit \\(\\mathbb{P}(K \\cap T)/\\mathbb{P}(T)\\) liefert und zur allgemeinen Definition der bedingten Wahrscheinlichkeit führt.\n\nDefinition 2.1 (Bedingte Wahrscheinlichkeit) Es seien \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) ein Wahrscheinlichkeitsraum und \\(A, B \\in \\mathcal{F}\\) seien zwei Ereignisse, wobei \\(\\mathbb{P}(B) > 0\\). Dann ist die bedingte Wahrscheinlichkeit von \\(A\\) unter der Bedingung \\(B\\) definiert als \\[\\begin{align*}\n    \\mathbb{P}(A \\vert B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}.\n\\end{align*}\\]\n\nIn diesem Zusammenhang bietet es sich auch an, dass Konzept von Unabhängigkeit zu erwähnen. Intuitiv ergibt es Sinn zwei Ereignisse als (stochastisch) unabhängig voneinander zu betrachen, wenn die Wahrscheinlichkeit, dass beide Ergebnisse eintreten, sich als Produkt der beiden Einzelwahrscheinlichkeiten ergibt. Formal definieren wir\n\nDefinition 2.2 (Unabhängigkeit) Es seien \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) ein Wahrscheinlichkeitsraum und \\(A, B \\in \\mathcal{F}\\) seien zwei Ereignisse. Die Ereignissee \\(A, B\\) sind genau dann unabhängig, falls \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\).\n\nVerknüpft man dies mit Definition 2.1 so stellt man fest, dass für unabhängige Mengen \\(A, B\\) gilt, dass \\(\\mathbb{P}(A \\vert B) = \\mathbb{P}(A)\\) und \\(\\mathbb{P}(B) = \\mathbb{P}(B \\vert A)\\). In der Praxis stellt man oft fest, dass wir zwar an einer bedingten Wahrscheinlichkeit \\(\\mathbb{P}(A \\vert B)\\) interessiert sind, aber nur die Wahrscheinlichkeit \\(\\mathbb{P}(B \\vert A)\\) messen können.\n\nBeispiel 2.1 Aus medizinischen Studien wissen wir, dass Rauchen das Risiko erhöht, an Lungenkrebs zu erkranken. Die untersuchte Größe ist hier also \\(\\mathbb{P}(K \\vert R)\\), wobei \\(K\\) für das Ereignis “Patient hat Krebs” und \\(R\\) für das Ereignis “Patient ist Raucher” steht. Jedoch können wir in Studien nur Krebspatienten als Raucher bzw. Nichtraucher identifizieren, d.h. durch unsere Beobachtungen erhalten wir nur Aufschluss über die Wahrscheinlichkeit \\(\\mathbb{P}(R \\vert K)\\).\n\nUm dennoch die gesuchte Größe zu berechnen, hilft uns der Satz von Bayes.\n\nTheorem 2.1 (Bayes) Es sei \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) ein Wahrscheinlichkeitsraum und \\(A, B \\in \\mathcal{F}\\) zwei Ereignisse mit \\(\\mathbb{P}(A), \\mathbb{P}(B) > 0\\). Dann gilt \\[\\begin{align*}\n    \\mathbb{P}(A \\vert B) = \\frac{\\mathbb{P}(A) \\mathbb{P}(B \\vert A)}{\\mathbb{P}(B)}.\n\\end{align*}\\]\n\nSomit stellt der Satz von Bayes, benannt nach dem Pfarrer Thomas Bayes, fest, dass die bedingten Wahrscheinlichkeiten proportional zueinander sind, wenn man Bedingung und zu untersuchendes Ereignis miteinander vertauscht. Zugleich liefert der Satz von Bayes den passenden Proportionalitätsfaktor dazu.\nEin letztes nützliches Werkzeug in diesem Zusammenhang ist der Satz über die totale Wahrscheinlichkeit. Dieser ermöglicht es, die Wahrscheinlichkeit eines Ereignisses durch die Zerlegung des Grundraumes als gewichtetes Mittel der zugehörigen bedingten Wahrscheinlichkeiten zu berechnen.\n\nTheorem 2.2 (Totale Wahrscheinlichkeit) Es sei \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) ein Wahrscheinlichkeitsraum und \\(A, B_1, B_2, \\ldots\\) seien Ereignisse mit \\(\\mathbb{P}(B_i) > 0\\) für alle \\(i \\in \\mathbb{N}\\). Außerdem sind die \\(B_i\\) paarweise disjunkt und \\(\\cup_{i \\in \\mathbb{N}} B_i = \\Omega\\). Dann gilt \\[\\begin{align*}\n        \\mathbb{P}(A) = \\sum_{i = 1}^{\\infty} \\mathbb{P}(A \\vert B_i)\\mathbb{P}(B_i).\n    \\end{align*}\\]\n\nNach dem Satz von Bayes gilt \\(\\mathbb{P}(A \\vert B) \\approx \\mathbb{P}(B \\vert A)\\) genau dann, wenn \\(\\mathbb{P}(A) \\approx \\mathbb{P}(B)\\). Das heißt im Allgemeinen ist es nicht möglich, die bedingte Wahrscheinlichkeit zu invertieren. Dennoch passiert diese Fehleinschätzung im Alltag so häufig, dass dieser logische Fehler auch als “Verwechslung der Umgekehrung”, “Umkehrungsfehlschluss” oder “Trugschluss des Anklägers” bekannt ist. Eins der bekanntesten Beispiele dafür beruht dabei auf einer Studie mit 100 Ärzten in der folgendes Fallbeispiel durchgespielt wurde:\n\nBeispiel 2.2 ((Eddy 1982)) Eine Patientin hat einen Knoten in der Brust woraufhin eine Mammographie durchgeführt wurde deren Befund zurückliefert, dass die Patienten einen bösartigen Tumor hat. Weiterhin sei bekannt, dass\n\n1% aller Brusttumore bösartig sind.\nMammographien einen bösartigen Tumor in 80 % der Fälle als solchen erkennen und in den sonstigen Fällen als gutartig einstufen.\nMammographien einen gutartigen Tumor in 90 % der Fälle als solchen erkennen und in den übrigen Fällen als bösartig einstufen.\n\nGesucht ist nun die bedingte Wahrscheinlichkeit \\(\\mathbb{P}(B \\vert P)\\), dass \\(B = \\text{``Tumor ist bösartig''}\\) gegeben \\(P = \\text{``Test auf bösartigen Tumor ist positiv''}\\). In der Studie haben fast alle der Ärzte diese Wahrscheinlichkeit auf ungefähr 75 % geschätzt und liegen damit sehr nahe an \\(\\mathbb{P}(P \\vert B) = 80 \\%\\). Die echte Wahrscheinlichkeit liefert der Satz von Bayes zusammen mit dem Satz der totalen Wahrscheinlichkeit als \\[\\begin{align*}\n    \\mathbb{P}(B \\vert P)\n    &=\n    \\frac{\\mathbb{P}(B) \\mathbb{P}(P \\vert B)}{\\mathbb{P}(P)}\\\\\n    &=\n    \\frac{\\mathbb{P}(B) \\mathbb{P}(P \\vert B)}{\n        \\mathbb{P}(P \\vert B) \\mathbb{P}(B) + \\mathbb{P}(P \\vert B^c) \\mathbb{P}(B^c)\n    }\\\\\n    &=\n    \\frac{0.01 \\cdot 0.80}{0.80 \\cdot 0.01 + 0.1 \\cdot 0.99}\n    =\n    0.0748.\n\\end{align*}\\]\n\n\nAnmerkung. Das vorangegangene Beispiel wurde als Verdeutlichung für logische Trugschlüsse aufgeführt. Aus aktuellen Anlässen sei an dieser Stelle erwähnt, dass aus diesem fiktiven Beispiel keine Rückschlüsse auf aktuelle medizinische Praxis in Bezug auf Mammographien oder sonstige Untersuchungen möglich ist.\n\n\n\n\n\nEddy, David M. 1982. „Probabilistic reasoning in clinical medicine: Problems and opportunities“. In Judgment under Uncertainty: Heuristics and Biases, herausgegeben von Daniel Kahneman, Paul Slovic, und AmosEditors Tversky, 249–67. Cambridge University Press. https://doi.org/10.1017/CBO9780511809477.019."
  },
  {
    "objectID": "bibliografie.html",
    "href": "bibliografie.html",
    "title": "Bibliografie",
    "section": "",
    "text": "Eddy, David M. 1982. “Probabilistic Reasoning in Clinical\nMedicine: Problems and Opportunities.” In Judgment Under\nUncertainty: Heuristics and Biases, edited by Daniel Kahneman, Paul\nSlovic, and AmosEditors Tversky, 249–67. Cambridge University Press. https://doi.org/10.1017/CBO9780511809477.019.\n\n\nSpodarev, Evgeny. 2018. Vorlesungsskript Zur Elementaren\nWahrscheinlichkeitsrechnung Und Statistik.\n\n\n———. 2020. Vorlesungsskript Zur Elementaren\nWahrscheinlichkeitsrechnung Und Stastistik."
  }
]